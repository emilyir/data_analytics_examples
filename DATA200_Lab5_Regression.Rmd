
---
title: "DATA-0200 Lab 5 - Dealing with Dirty Data & Regression analysis"
author: "Kyle Monahan, based on the original tutorial by Grant R. McDermott"
output:
  pdf_document:
    toc: yes
  word_document:
    toc: yes
  html_document:
    toc: yes
  always_allow_html: true
---


```{r setup, include=FALSE}
if (!require("knitr")) 
install.packages("knitr")
library(knitr)
knitr::opts_chunk$set(echo = TRUE, cache = TRUE, dpi=500)

# Modified by Emily with this note here!

# These are knitr options, which is a package in R. Echo tells us that we will receive responses when running this notebook, caches saves the objects as we run them, and dpi sets the dots per inch or resolution of our figures.

```


## A note 

Note that this is based on a lecture by the excellent Grant R. McDermott. Please **don't** reach out to him with questions, ask a Lab Assistant or myself via the email list  **DataLab-Support@elist.tufts.edu** if you have questions. I've modified this approach based on your previous three lessons, and the materials I created for you for the other lessons. 

## R notebooks

Today we will use a notebook, with a lot of additional functionality. We will continue to use notebooks for our R code moving forward, so feel free to try them out on your own.

## Today's workshop

Today's lecture is about the bread-and-butter tool of applied econometrics and data science: regression analysis. My goal is to give you a whirlwind tour of the key functions and packages. I'm going to assume that you already know all of the necessary theoretical background on causal inference, asymptotics, etc. This lecture will *not* cover any of theoretical concepts or seek to justify a particular statistical model. Indeed, most of the models that we're going to run today are pretty silly. We also won't be able to cover some important topics. For example, I'll only provide the briefest example of a Bayesian regression model and I won't touch times series analysis at all. 


### Follow up on MATLAB

But suffice to say that MATLAB is excellent if you want to perform signal processing or work with large matricies. Image manipulation, as it is a function of large matricies of pixel values, is particularly good in MATLAB. 

If you want to learn more about installing and using MATLAB, see the documentation I created here: https://tufts.box.com/v/UsingInstallingMATLAB


## Software requirements

### R packages 

It's important to note that "base" R already provides all of the tools we need for basic regression analysis. However, we'll be using several external packages today, because they will make our lives easier and offer increased power for some more sophisticated analyses.

>>> Remember that you can tell if a package is in base R by specifying it as base::function() or looking for information via library: `library(help="base")`

Base R provides a bunch of functions, so why do we need to use all these other packages? Well, normally they provide additional functions or more convienient options. 


### New packages

- **New:** `broom`, `estimatr`, `sandwich`, `lmtest`, `AER`, `lfe`, `plm`, `huxtable`, `margins`,`hrbrthemes`, `listviewer`

- **Already used:** `tidyverse`

The `broom` package was bundled with the rest of tidyverse and `sandwich` should get installed as a dependency of several of the above packages.

Still, a convenient way to install (if necessary) and load everything is by running the below code chunk. I'll also go ahead and set my preferred ggplot2 theme for the rest of this document.



```{r, cache=F, message=F}
#Using pacman to install these packages https://www.rdocumentation.org/packages/pacman/versions/0.5.1

# I sometimes prefer packrat but this is certainly lighter to use pacman.

# The code below means, install this (pacman) if it isn't installed already

# The ! operator is boolean not AKA, if not require pacman is true (or if pacman is not installed), then install pacman

if (!require("pacman")) install.packages("pacman")
pacman::p_load(tidyverse, broom, hrbrthemes, plm, estimatr, sandwich, lmtest, AER, lfe, huxtable, margins,listviewer, rstanarm)

# Set theme
# my_theme <- theme_set(theme_grey())
# theme_set(my_theme)
#theme_set(hrbrthemes::theme_ipsum(base_family = "Arial Narrow")) 
#You have quite a few options here: https://github.com/hrbrmstr/hrbrthemes


```



```{r cache=F, include=F}
## Switch off automatic "pretty printing" of DFs
## See: https://hughjonesd.github.io/huxtable/huxtable.html#automatic-pretty-printing-of-data-frames
options(huxtable.knit_print_df = FALSE)

```

While we've already loaded all of the required packages for today, I'll try to be as explicit about where a particular function is coming from, whenever I use it below. 

Something else that I want to mention up front is that we'll mostly be working with the `starwars` data frame. This consists of data about the characters in the feature film "Star Wars."

I don't condone the fact the gender is only male/female, especially since we know the pirate ruler Eleodie Maracavanya identified as "zhe."

https://starwars.fandom.com/wiki/Eleodie_Maracavanya

Let's take a look.

```{r starwars}
starwars
```


## Regression basics - estimation

### The `lm()` function 

R's workhorse command for running regression models is the built-in `lm()` function. The "**lm**" stands for "**l**inear **m**odels" and the syntax is very intuitive.^[Indeed, all other regression packages in R that I'm aware of --- including those that allow for much more advanced and flexible models --- closely follow the `lm()` syntax.] 

This means that once you learn lm(), you'll be able to learn many other regression packages in R. 

```{r}
# The syntax is lm(outcome ~ pred1 + pred2 + pred3 and so on, data= data.df)
# `lm(y ~ x1 + x2 + x3 + ..., data = df)`
```



You'll note that the `lm()` call includes a reference to the data source (in this case, a hyopthetical data frame called `df`).

As many data frames can exist in your R environment at the same time, we have to specifically call out which data frame we want to use. 

For example, when we looped over the housing data, we created many datasets. 

>>> PROTIP: Comparing to Stata, where only one dataset is allowed at a time, where we only have to specify a single variable. 

So we need to be specific about where our regression variables are coming from --- even if `df` is the only data frame in our global environment at the time. Another option would be to use column indexing with the `$` operator, but I find it a bit verbose:

```r
lm(df$y ~ df$x1 + df$x2 + df$x3 + ...)
```

>>> Let's say we wanted to learn more about lm(). How can we do this? Hint - see the most important slide! 

Let's run a simple bivariate regression of starwars characters' mass on height.


```{r ols1}
ols1 <- lm(mass ~ height, data = starwars)
# ols1 <- lm(starwars$mass ~ starwars$height) ## Also works
ols1
```

>>> This is answering the question - how does knowing a Star War's characters mass inform our knowledge of their height? 

The resulting object is pretty terse, but that's only because it buries most of its valuable information --- of which there is a lot --- within its internal list structure. You can use the `str()` function to view this structure. Or, if you want to be fancy, the interactive `listviewer::jsonedit()` function is nice. 

>> NOTE: JSON stands for Java Script Object Notation. This is a way to store nested objects in a nice way. It's actually one of the ways that we can access nested strings as well. 

```{r ols1_str, message=F, out.width="100%", out.height="10%"}
# str(ols1) ## Static option
listviewer::jsonedit(ols1, mode="view") ## Interactive option
```



As we can see, this `ols1` object has a bunch of important slots... containing everything from the regression coefficients, to vectors of the residuals and fitted (i.e. predicted) values, to the rank of the design matrix, to the input data, etc. etc. 

>> Note how this compares to Stata, where we see very verbose output initially. We can also call another function to produce similar output: 


To summarise the key pieces of information, we can use the --- *wait for it* --- generic `summary()` function. This will look pretty similar to the default regression output from Stata that many of you will be used to.


```{r ols1_summ}
summary(ols1)
```


>>> PROTIP: This is very similar to the output on page 45 of the lecture notes on Regression Estimation. When Dr. Zabel says that "A man with BA degree earns $12.13 more per hour than a man
with at most a high school degree, on average and all else constant" in our model here we can say that For every one unit change in the height of a character, we would expect a 0.6386 unit change in the weight of the character, all else constant. However, because the p-value is not significant (Pr(>|t|) = 0.312) and the overall model p-value reporting similar I would worry abou this model. Greatly. 

We can then dig down further by extracting a summary of the regression coefficients:


```{r ols1_coefs}
summary(ols1)$coefficients
```


### Get "tidy" regression coefficients with the `broom` package


While it's easy to extract regression coefficients via the `summary()` function, in practice one can commonly use the [broom package](https://broom.tidyverse.org/) to do so. This package has a bunch of neat features to convert regression (and other statistical) objects into "tidy" data frames. This is especially useful because regression output is so often used as an input to something else, e.g. a plot of coefficients or marginal effects. Here, I'll use `broom::tidy(..., conf.int=T)` to coerce the `ols1` regression object into a tidy data frame of coefficient values and key statistics.


```{r ols1_tidy}
library(broom)
tidy(ols1, conf.int = T)
```


Again, I could now pipe this tidied coeffients data frame to a ggplot2 call, using saying `geom_pointrange()` to plot the error bars. Feel free to practice doing this yourself now, but we'll get to some explicit examples further below.

>>> PROTIP: This is actually a common issue which we mentioned during my first lecture with you - how do I know what format the function would like? For example, if you look at geom_pointrange() you'll see it requires specific formatting of your inputs. This can be challenging, but just like when we used `class()` to investigate the data frame object, we can do the same thing for the objects or values placed inside functions. 


A related and also useful function is `broom::glance()`, which summarises the model "meta" data (R<sup>2</sup>, AIC, etc.) in a data frame.

I really like glance and use this often! 

```{r ols1_glance}
glance(ols1)
```
(BTW, If you're wondering how to export regression results to other formats (e.g. LaTeX tables), don't worry: We'll get to that at the very end of the lecture.)


### Regressing on subsetted or different data
Our simple model isn't particularly good; our R squared is only `r I(round(glance(ols1)$r.squared, 3))`. Different species and homeworlds aside, we may have an extreme outlier in our midst...


```{r jabba, message=F}
starwars %>%
  ggplot(aes(x=height, y=mass)) +
  geom_point(alpha=0.5) +
  geom_point(
    data = starwars %>% filter(mass==max(mass, na.rm=T)), 
    col="red"
    ) +
  geom_text(
    aes(label=name),
    data = starwars %>% filter(mass==max(mass, na.rm=T)), 
    col="red", vjust = 0, nudge_y = 25
    ) +
  labs(
    title = "Spot the outlier...",
    caption = "Aside: Always plot your data!"
    )
```
Maybe we should exclude Jabba from our regression? You can do this in two ways: 1) Create a new data frame and then regress, or 2) Subset the original data frame directly in the `lm()` call.

>>> Breakout Groups: Find one other measure of outliers.

#### 1) Create a new data frame
Recall that we can keep multiple objects in memory in R. So we can easily create a new data frame that excludes Jabba using `dplyr::filter()`. I prefer this approach sense it is easier to follow when you return to the ...


>>> Note: Take a look at the dplyr data manipulation cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-transformation.pdf

>>> Also check out the ggplot() cheat sheet: https://github.com/rstudio/cheatsheets/raw/master/data-visualization-2.1.pdf


Back to the analysis - we can use the pipe from `dplyr` to do this. Remember, this is the same as the | symbol in bash/terminal and the pipe in F# and Haskell via (>->) for yield or await commands. 


```{r ols2}
starwars2 <-
  starwars %>% 
  filter(name != "Jabba Desilijic Tiure")
  # filter(!(grepl("Jabba", name))) ## Regular expressions also work
ols2 <- lm(mass ~ height, data = starwars2)
summary(ols2)
```



#### 2) Subset directly in the `lm()` call
Running a regression directly on a subsetted data frame is equally easy.


```{r ols2a}
ols2a <- lm(mass ~ height, data = starwars %>% filter(!(grepl("Jabba", name))))
summary(ols2a)
```


The overall model fit is much improved by the exclusion of this outlier, with R<sup>2</sup> increasing to `r I(round(glance(ols2)$r.squared, 3))`. Still, we should be cautious about throwing out data. Another approach is to handle or account for outliers with statistical methods. Which provides a nice segue to robust and clustered standard errors.



## Robust and clustered standard errors

Dealing with statistical irregularities (heteroskedasticity, clustering, etc.) is a fact of life for empirical researchers. However, it says something about the economics profession that a random stranger could walk uninvited into a live seminar and ask, "How did you cluster your standard errors?", and it would likely draw approving nods from audience members. 

>>> When to cluster errors? In two main cases: 1) When the sample collection produced clusters in the population of interest that are not in the sample of that population, or when your experiment was designed in such a way to assign people to a specific group or outcome based on their cluster status. More information here: https://blogs.worldbank.org/impactevaluations/when-should-you-cluster-standard-errors-new-wisdom-econometrics-oracle

The good news is that there are *lots* of ways to get robust and clustered standard errors in R. For many years, these have been based on the excellent [sandwich package](https://cran.r-project.org/web/packages/sandwich/index.html). 


However, my prefered way these days is to use the [`estimatr` package](https://declaredesign.org/r/estimatr/articles/getting-started.html), which is both fast and provides convenient aliases for the standard regression functions. For example, you can obtain robust standard errors using `estimatr::lm_robust()`. Let's illustrate by running a robust version of the `ols1` regression that ran earlier.


```{r ols1_robust}
# library(estimatr) ## Already loaded
ols1_robust <- lm_robust(mass ~ height, data = starwars)
tidy(ols1_robust, conf.int = T)
```


The package defaults to using Eicker-Huber-White robust standard errors, commonly referred to as "HC2" standard errors. You can easily specify alternate methods using the `se_type = ` argument.^[See the [package documentation](https://declaredesign.org/r/estimatr/articles/mathematical-notes.html#lm_robust-notes) for a full list of options.] For example, you can specify Stata robust standard errors if you want to replicate code or results from that language. (See [here](https://declaredesign.org/r/estimatr/articles/stata-wls-hat.html) for more details on why this isn't the default and why Stata's robust standard errors differ from those in R and Python.)


```{r ols1_robust_stata}
ols1_robust_stata <- lm_robust(mass ~ height, data = starwars, se_type = "stata")
tidy(ols1_robust_stata, conf.int = T)
```


This is really nice for 1) getting some of the same output you might see in your lecture notes and 2) impressing your Stata friends with different approaches to finding SE within the same code!

>>> PROTIP: Check out page 31 of the MLRM: Inference for the reg y x, vce command, which vce(robust) requesting standard errors in Stata. 

The `estimatr` package also supports (robust) instrumental variable regression and clustered standard errors. I'll return to these issues in the relevant sections below, but here's a quick example of the latter just to illustrate:


```{r ols1_robust_clustered}
ols1_robust_clustered <- lm_robust(mass ~ height, data = starwars, clusters = homeworld)
tidy(ols1_robust_clustered, conf.int = T)
```

>>> BREAKOUT Groups: In groups, find another package which allows for reporting of robust standard errors. 

### Aside on HAC (Newey-West) standard errors
On thing I want to flag is that the `estimatr` package does not yet offer support for HAC (i.e. heteroskedasticity and autocorrelation consistent) standard errors *a la* 
[Newey-West](https://en.wikipedia.org/wiki/Newey%E2%80%93West_estimator) - you can still obtain these pretty easily using the aforementioned `sandwich` package. For example, we can use `sandwich::NeweyWest()` on our existing `ols1` object to obtain HAC SEs for it.


```{r ols1_hac}
# library(sandwich) ## Already loaded
NeweyWest(ols1) ## Print the HAC VCOV
sqrt(diag(NeweyWest(ols1))) ## Print the HAC SEs
```


If you wanted to convert it to a tidy data frame of coefficient values, then, I would recommend first piping it to `lmtest::coeftest(..., vcov=NeweyWest)`, which is a convenient way to do hypothesis testing using alternate variance-covariance matrices. Note that in the below, I'm going to manually create my own upper and lower 95% confidence intervals, since `broom::tidy(conf.int=T)` doesn't work with coeftest objects.


```{r ols1_hac_tidy}
# library(lmtest) ## Already loaded
ols1 %>% 
  lmtest::coeftest(vcov=NeweyWest) %>%
  tidy() %>% ## "conf.int" doesn't work with coeftest object, so calculate manually...
  mutate(
    conf.low = estimate - qt(0.975, df=ols1$df.residual)*std.error,
    conf.high = estimate + qt(0.975, df=ols1$df.residual)*std.error
    )
```

This sort of approach of using mutate to calculate confidence intervals manually and passing them into the final data frame is a classic intermediate R issue - one package does things one way, and another does things another way. A lot of time it's about reading and understanding the documentation, rather than automatically knowing how things fit together. So if you're reading a lot of code and documentation, you're probably on the right track! 


## Dummy variables and interaction terms
### Dummy variables as *factors*


Dummy variables are a core component of many regression models. However, these can be a pain to create in many statistical languages, since you first have to tabulate a whole new matrix of binary variables and then append it to the original data frame. In contrast, R has a much more convenient framework for creating and evaluating dummy variables in a regression. You simply specify the variable of interest as a [factor](https://r4ds.had.co.nz/factors.html).^[Factors are variables that have distinct qualitative levels, e.g. "adult", "child", "elder", etc.]

>>> PROTIP: This is very similar to the factor approach in Stata: https://www.stata.com/features/overview/factor-variables/ Also remember that creating dummy variables in Stata achives a similar end: https://www.stata.com/support/faqs/data-management/creating-dummy-variables/

For this next section, it will be convenient to demonstrate using a subsample of the data that comprises only humans. I'll first create this `humans` data frame and then demonstrate the dummy-variables-as-factors approach.
 
```{r ols_dv}
humans <- 
  starwars %>% 
  filter(species=="Human") %>%
  mutate(gender_factored = as.factor(gender)) %>% ## create factored version of "gender"
  select(contains("gender"), everything())
humans
ols_dv <- lm(mass ~ height + gender_factored, data = humans)
summary(ols_dv)
```


In fact, I'm even making things more complicated than they need to be. R is "friendly" and tries to help whenever it thinks you have misspecified a function or variable. While this is something to be [aware of](https://rawgit.com/grantmcdermott/R-intro/master/rIntro.html#r_tries_to_guess_what_you_meant), it normally just works<sup>TM</sup>. A case in point is that we don't actually *need* to specify a qualitative or character variable as a factor in a regression. R will automatically do this for you regardless, since that's the only sensible way to include string variables in a regression.

>>> PROTIP: The power of this should not be underestimated. In SAS, we would have to re-store all this data as factors in order to work with it. This would be such a pain, and it's so much nicer to have R take care of it in the backend (most of the time).

```{r ols_dv2}
## Use the non-factored "gender" variable instead
ols_dv2 <- lm(mass ~ height + gender, data = humans)
summary(ols_dv2)
```


### Interaction effects
Like dummy variables, R provides a convenient syntax for specifying interaction terms directly in the regression model without having to create them manually beforehand.^[Although there are very good reasons that you might want to modify your parent variables before doing so (e.g. centering them).

You can just use `x1:x2` (to include only the interaction term) or `x1*x2` (to include the parent terms and interaction terms). Generally speaking, you are best advised to include the parent terms alongside an interaction term. This makes the `*` option a good default. 
For example, we might wonder whether the relationship between a person's body mass and their height is modulated by their gender. That is, we want to run a regression of the form
$$Mass = \beta_0 + \beta_1 D_{Male} + \beta_2 Height + \beta_3 D_{Male} \times Height$$
To implement this in R, we simply run the following
```{r ols_ie}
ols_ie <- lm(mass ~ gender*height, data = humans)
summary(ols_ie)
```

>>> PROTIP: This is referred to as "interaction terms" on page 40 of your PDF on MLRM: Estimation. 

>>> BREAKOUT Groups: In groups, suggest and add another interaction term. State why you selected that interaction term, and if it was significant. 

## Panel models
### Fixed effects with the `lfe` package
The simplest (and least efficient) way to include fixed effects in a regression model is, of course, to use dummy variables. However, it isn't very efficient or scaleable. What's the point learning all that stuff about the Frisch-Waugh-Lovell theorem, within-group transformations, etcetera, etcetera if we can't use them in our software routines? 

Again, there are several options to choose from here. For example, the venerable [plm package](https://cran.r-project.org/web//packages/plm/vignettes/plmPackage.html), which also handles random effects and pooling models. However, I am going to strongly advocate for the [lfe package](https://cran.r-project.org/web/packages/lfe/index.html).
`lfe` (i.e. "**l**inear **f**ixed **e**ffects") is one of my packages in the entire R catalogue. 


It has a boatload of functionality built in to it (instrumental variables support, multilevel clustering, etc.) It is also *fast* because it automatically uses all the available processing power on your machine. For the moment, simply enjoy the fact that `lfe` is optimised to solve big regression problems as quickly as possible.
Let's take a look, starting off with a simple example and then moving on to something more demanding.

>>> Note: This is also the library I suggest you use if you are looking to run R on the high-performance compute cluster. Also note, check out rstudio.cloud for a nice browser instance (but remember it's a *nix box - AKA a linux server and you don't have root!)


#### Simple FE model
The package's main function is `lfe::felm()`, which is used for estimating fixed effects linear models. The syntax is such that you first specify the regression model as per normal, and then list the fixed effect(s) after a `|`. An example may help to illustrate. Let's say that we again want to run our simple regression of mass on height, but this time control for species-level fixed effects.

>>> Note: This | is normally a pipe in other languages, but here it is acting as a seperator.

```{r ols_fe, message=FALSE}
library(lfe)
ols_fe <- felm(mass ~ height | species, data = starwars) ## Fixed effect(s) go after the "|"
coefs_fe <- tidy(ols_fe, conf.int = T)
summary(ols_fe)
```
Note that the resulting `felm` object drops all of the species intercepts, since it has abstracted them away as fixed effects.



#### High dimensional FEs and (multiway) clustering
One reason that I prefer the `lfe` package to other options --- e.g. the panel-focused `plm` package (see further below) --- is because it supports high dimensional fixed effects *and* (multiway) clustering.^[It is very similar to the excellent [reghdfe](http://scorreia.com/software/reghdfe/) package in Stata.] 

In the below example, I'm going to add "homeworld" as an additional fixed effect to the model and also cluster according to this variable. I'm not claiming that this is a particularly good or sensible model, but just go with it. Note that, since we specify "homeworld" in the fixed effects slot below, `felm()` automatically converts it to a factor even though we didn't explicitly tell it to.


```{r ols_hdfe}
ols_hdfe <- 
  felm(
    mass ~ height |
      species + homeworld | ## Two fixed effects go here after the first "|"
      0 | ## This is where your IV equation goes, but we put 0 since we aren't instrumenting.
      homeworld, ## The final slot is where we specify our cluster variables
    data = starwars)
coefs_hdfe <- tidy(ols_hdfe, conf.int = T)
coefs_hdfe
```


Visually, we can easily compare changes in the coefficients across models thanks to the fact that we saved the output in data frames with `broom::tidy()` above.


```{r fe_mods_compared}
bind_rows(
  coefs_fe %>% mutate(reg = "Model 4 (FE and no clustering)"),
  coefs_hdfe %>% mutate(reg = "Model 5 (HDFE and clustering)")
  ) %>%
  ggplot(aes(x=reg, y=estimate, ymin=conf.low, ymax=conf.high)) +
  geom_pointrange() +
  labs(Title = "Marginal effect of height on mass") +
  geom_hline(yintercept = 0, col = "orange") +
  ylim(-0.5, NA) +
  labs(
    title = "'Effect' of height on mass",
    caption = "Data: Characters from the Star Wars universe"
    ) +
  theme(axis.title.x = element_blank())
```
Normally we expect our standard errors to blow up with clustering, but here that effect appears to be outweighted by the increased precision brought on by additional fixed effects. (As suggested earlier, our level of clustering probably doesn't make much sense either.)


#### Instrumental variables
(See further below.)

### Random effects
Fixed effects models are more common than random effects models in economics (in my experience, anyway).


>>> Note: As someone who assists faculty across departments, you'll find that different fields use the term "fixed effects" differently. See some excellent comments by Gelman here: https://statmodeling.stat.columbia.edu/2005/01/25/why_i_dont_use/ 


I'd also advocate for [Bayesian hierachical models](http://www.stat.columbia.edu/~gelman/arm/) if we're going down the whole random effects path. However, it's still good to know that R has you covered for random effects models through the the [plm](https://cran.r-project.org/web/packages/plm/) and [nlme](https://cran.r-project.org/web/packages/nlme/index.html) packages.^[As I mentioned above, `plm` also handles fixed effects (and pooling) models. However, I prefer `lfe` for the reasons already discussed.] I won't go into detail , but click on those links (especially the first one) if you would like to see some examples.


## Instrumental variables
As you would have guessed by now, there are a number of ways to run instrumental variable (IV) regressions in R. I'll walk through three options using the `AER::ivreg()`, `estimatr::iv_robust()`, and `lfe::felm()` functions, respectively. These are all going to follow a similar syntax, where the IV first-stage regression is specified after a **`|`** following the main regression. However, there are also some subtle and important differences, which is why I want to go through each of them. After that, I'll let you decide which of the three options is your favourite.

The dataset that we'll be using here is a panel of US cigarette consumption by state, which is taken from the [AER package](https://cran.r-project.org/web/packages/AER/vignettes/AER.pdf). Let's load the data, add some modified variables, and then take a quick look at it. Note that I'm going to limit the dataset to 2005 only, given that I want to focus the IV syntax and don't want to deal with the panel structure of the data. (Though that's very easily done, as we've already seen.)

```{r, cigs}
## Get the data
data("CigarettesSW", package = "AER") # Note this is example data from the AER package 

## Create a new data frame with some modified variables
cigs <-
  CigarettesSW %>%
  mutate(
    rprice = price/cpi,
    rincome = income/population/cpi,
    rtax = tax/cpi,
    tdiff = (taxs - tax)/cpi
    ) %>%
  as_tibble()
## Create a subset of the data limited to 1995
cigs95 <- cigs %>% filter(year==1995)
cigs95
```

Now, assume that we are interested in regressing the number of cigarettes packs consumed per capita on their average price and people's real incomes. The problem is that the price is endogenous (because it is simultaneously determined by demand and supply), so we need to instrument for it using different tax variables. That is, we want to run the following:

$$price_i = \pi_0 + \pi_1 tdiff_i + + \pi_2 rtax_i + v_i  \hspace{1cm} \text{(First stage)}$$
$$packs_i = \beta_0 + \beta_2\widehat{price_i} + \beta_1 rincome_i + u_i \hspace{1cm} \text{(Second stage)}$$

### Option 1: `AER::ivreg()`

Let's start with `AER::ivreg()` as our first IV regression option; if for no other reason than that's where our data are coming from. The key point from the below code chunk is that the first-stage regression is going to be specified after the **`|`** and will include *all* exogenous variables.

```{r, iv_reg}
# library(AER) ## Already loaded
## Run the IV regression 
iv_reg <- 
  ivreg(
    log(packs) ~ log(rprice) + log(rincome) | ## The main regression. "rprice" is endogenous
      log(rincome) + tdiff + rtax, ## List all exogenous variables, including "rincome"
    data = cigs95
    )
summary(iv_reg, diagnostics = TRUE)
```

I want to emphasise that you might find the above syntax a little counterintuitive --- or, at least, unusual --- if you're coming from a language like Stata.^[Assuming that you have already created the logged variables and subsetted the data, the Stata equivalent would be something like `ivreg log_packs = log_rincome (log_rprice = tdiff rtax)`.] Note that we didn't specify the endogenous variable (i.e. "rprice") directly. Rather, we told R which are the *exogenous* variables. It then figured out which were the endogenous variables that needed to be instrumented and ran the necessary first-stage regression(s) in the background. This approach actually makes quite a lot of sense if you think about the underlying theory of IV. But different strokes for different folks. 

The good news for those who prefer the Stata-style syntax is that `AER::ivreg()` also accepts an alternate way of specifying the first-stage. This time, we'll denote our endogenous "rprice" variable with `. -price` and include only the instrumental variables after the `|` break. Feel free to check yourself, but the outcome will be exactly the same.

```{r iv_reg2}
## Run the IV regression 
iv_reg2 <- 
  ivreg(
    log(packs) ~ log(rprice) + log(rincome) | 
      . -log(rprice) + tdiff + rtax, ## Alternative way of specifying the first-stage.
    data = cigs95
  )
```


### Option 2: `estimatr::iv_robust()`

Our second IV option comes from the `estimatr` package that we saw earlier. This will default to using HC2 robust standard errors although, as before, we could specify other options if we so wished (including clustering). More importantly, note that the syntax is effectively identical to the previous example. All we need to do is change the function call from `AER::ivreg()` to `estimatr::iv_robust()`.

```{r, iv_robust}
# library(estimatr) ## Already loaded
## Run the IV regression with robust SEs
iv_reg_robust <- 
  iv_robust( ## We only need to change the function call. Everything else stays the same.
    log(packs) ~ log(rprice) + log(rincome) | 
      log(rincome) + tdiff + rtax,
    data = cigs95
    )
summary(iv_reg_robust, diagnostics = TRUE)
```

### Option 3: `felm::lfe()`

Finally, we get to my personal favourite IV option using the `lfe::felm()` function that we already covered in the panel data section above.^[Needless to say, it includes all of same benefits that we saw earlier: support for high-level fixed effects, multiway clustering, etc.] It's my favourite option not because I tend work with panel data, but also because I find it has the most natural syntax. In fact, it very closely resembles Stata's approach to writing out the first-stage, where you specify the endogenous variable(s) and the instruments only.

```{r iv_felm}
# library(lfe) ## Already loaded
iv_felm <- 
  felm(
    log(packs) ~ log(rincome) |
      0 | ## No FEs
      (log(rprice) ~ tdiff + rtax), ## First-stage. Note the surrounding parentheses
    data = cigs95
  )
summary(iv_felm)
```

>>> PROTIP: Having trouble interpreting how changing the model impacts how we interpret the results? I suggest you check out IDRE from UCLA: https://stats.idre.ucla.edu/other/dae/

Note that in the above example, we inserted a "0" where the fixed effect slot goes, since we only used a subset of the data. Just for fun then, here's another IV regression with `felm()`. This time, I'll use the whole `cigs` data frame (i.e. not subsetting to 1995), and use both year and state fixed effects to control for the panel structure.

```{r iv_felm_all}
iv_felm_all <- 
  felm(
    log(packs) ~ log(rincome) |
      year + state | ## Now include FEs
      (log(rprice) ~ tdiff + rtax), 
    data = cigs ## Use whole panel data set
  )
summary(iv_felm_all)
```

## Other topics

### Marginal effects

Caculating marginal effect in a regression is utterly straightforward in cases where there are no non-linearities; just look at the coefficient values! However, that quickly goes out the window when you have interaction effects or non-linear models like probit, logit, etc. Luckily, the `margins` package (which is modeled on its namesake in Stata) goes a long way towards automating the process. You can read more in the [package vignette](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html), but here's a very simple example to illustrate. 

Consider our earlier interaction effects regression, where we interested in how people's mass varied by height and gender. To get the average marginal effect (AME) of these dependent variables, we can just use the `margins::margins()` function.

```{r margins0}
# library(margins) ## Already loaded
margins(ols_ie)
```


>>> PROTIP: In Stata, this would be achived via something like: `margins <enter> marginsplot`

You can get standard errors by piping (or wrapping) the above object to the generic `summary()` function. 

```{r margins1}
# summary(margins(ols_ie)) ## Also works
ols_ie %>% margins() %>% summary() 
```

If we want to compare marginal effects at specific values --- e.g. how the AME of height on mass differs across genders --- then that's easily done too.

```{r margins2}
#ols_ie %>% 
#  margins(
#    variables = "height", ## The main variable we're interested in
#    at = list(gender = c("male", "female")) ## How the main variable is modulated by at specific values of a second variable
#    ) #%>% 
  # summary() ## If you want SEs etc.
```

If you're the type of person who prefers visualizations (like me), then you should consider `margins::cplot()`, which is the package's in-built method for constructing *conditional* effect plots.

```{r margins3}
cplot(ols_ie, x = "gender", dx = "height", what = "effect")
```

In this case,it doesn't make much sense to read a lot into the larger standard errors on the female group; that's being driven by a very small sub-sample size.

>>> Note: This is really similar to the approach in Stata - they are both showing the marginal effect of the variables on the outcome (height). See details on Stata interpretation in your notes and here: https://www3.nd.edu/~rwilliam/stats/Margins01.pdf

Finally, you can also use `cplot()` to plot the predicted values of your outcome variable (here: "mass"), conditional on one of your dependent variables. For example:

```{r margins4}
par(mfrow=c(1, 2)) ## Just to plot these next two (base) figures side-by-side
cplot(ols_ie, x = "gender", what = "prediction")
cplot(ols_ie, x = "height", what = "prediction")
par(mfrow=c(1, 1)) ## Reset plot defaults
```

Note that `cplot` automatically produces a data frame of the predicted effects too. This can be used to construct `ggplot2` versions of the figures instead of the (base) `cplot` defaults. See the package documentation for [more information](https://cran.r-project.org/web/packages/margins/vignettes/Introduction.html#ggplot2_examples).

**Aside:** One downside that I want to highlight briefly is that the `margins` package does [not yet work](https://github.com/leeper/margins/issues/73) with `lfe::felm` objects. There are [potential ways](https://stackoverflow.com/questions/30491545/predict-method-for-felm-from-lfe-package) around this, or you can just calculate the marginal effects manually, but it's admittedly a pain.

### Probit, logit and other generalized linear models

See `?stats::glm`.

### Synthetic control

See the [gsynth package](https://yiqingxu.org/software/gsynth/gsynth_examples.html).

### Bayesian regression

We could spend a whole course on Bayesian models. The very, very short version is that R offers outstanding support for Bayesian models and data analysis. You will find convenient interfaces to all of the major MCMC and Bayesian software engines: [Stan](https://mc-stan.org/users/interfaces/rstan), [JAGS](http://mcmc-jags.sourceforge.net/), TensorFlow (via [Greta](https://greta-stats.org/)), etc. 


Here follows a *super* simple example using the [rstanarm package](http://mc-stan.org/rstanarm/). Note that we did not install this package with the others above, as it can take fairly long and involve some minor troubleshooting.^[FWIW, on my machine (running Arch Linux) I had to install `stan` (and thus `rstanarm`) by running R through the shell. For some reason, RStudio kept closing midway through the installation process.]


>>> PROTIP: On Macs, this is also an issue, at least in Early Oct. 2020 when I attempted to update stan on a new image. But you can eventually make it through.

```{r bayes_reg, error=T, message=F, warning=F, results="hide"}
# install.packages("rstanarm") ## Run this first if you want to try yourself
library(rstanarm)
bayes_reg <- 
  stan_glm(
    mass ~ gender*height,
    data = humans, 
    family = gaussian(), prior = cauchy(), prior_intercept = cauchy()
    )
```
```{r bayes_reg_summ, error=T}
summary(bayes_reg)
tidy(bayes_reg)
```


### Visualizing regression output and models

We've already worked through several visualization examples today and you should all be familiar with ggplot2's `geom_smooth()` from our earlier lectures. For instance:

```{r smooth, warning=F}
humans %>%
  ggplot(aes(x=mass, y=height, col=gender)) + 
  geom_point(alpha=0.7) +
  geom_smooth(method="lm", se=F) + ## See ?geom_smooth for other methods
  scale_color_brewer(palette = "Set1")
```

For further reference, I highly encourage you to look over Chapter 6 of Kieran Healy's [*Data Visualization: A Practical Guide*](https://socviz.co/modeling.html#plot-marginal-effects). You will not only learn how to produce beautiful and effective model visualizations, but also pick up a variety of technical tips. 

You may want to pay particular attention attention to the section on [generating and plotting predictions](https://socviz.co/modeling.html#generate-predictions-to-graph).

### Exporting regression results and descriptive tables (LaTeX, etc.)

There are a loads of different options here. I've historically favoured the [stargazer package](https://www.jakeruss.com/cheatsheets/stargazer/), but I also like the [huxtable package](https://hughjonesd.github.io/huxtable). Then there's the new [gt package](https://gt.rstudio.com/index.html) from the RStudio team, which is drawing lots of attention. And this is just a small sample of the available options; see [here](https://hughjonesd.github.io/huxtable/design-principles.html) for a handy comparison of different table "engines" in R.

Here follows a bare-bones example using huxtable, since it works well with R Markdown documents.

```{r hux, message=F}
library(huxtable)
huxreg(ols_dv, ols_ie, ols_hdfe)
```



If you look at what I sent out previously, you'll see I took a similar approach: 


```{r}
# Install the following packages:
#install.packages("rockchalk")
#install.packages("jtools")
#install.packages("huxtable")
#install.packages("tidyverse")
#install.packages("officer")
#install.packages("flextable")
#install.packages("kable")
#install.packages("summarytools")

#For table of summary statistics in word
library(summarytools)
library(officer)
library(flextable)

outtable <- summarytools::descr(humans, transpose = TRUE, format="html", round.digits=3, stats =
c("mean", "sd", "min", "max"))

finaltab <- flextable(outtable)

fint <- autofit(finaltab)

print(fint, preview = "docx")

#For regression results in word
#jtools::export_summs(fint, to.file = "docx", file.name = "test2.docx", number_format = "%.3f")

```

## Regression - You try

Let's say we wanted to create our own regression model. Pulling another dataset from the example data in R, we can use the **EquationCitations** dataset. From the documentation: "Fawcett and Higginson (2012) investigate the **relationship between the number of citations evolutionary biology papers receive, depending on the number of equations per page in the cited paper**. Overall it can be shown that papers with many mathematical equations significantly lower the number of citations they receive, in particular from nontheoretical papers."

So we may want to investigate, using regression, how well the number of equations predicts the number of citations that a paper receives. Beyond the potential lag effect between number of equations and citations, and the lower likelyhood that this holds true in other equation-heavy yields, what can we do to address this question?

```{r}
# Request the built-in data on citation

data("EquationCitations", package = "AER")

# Our suggested outcome is cites (citations), and your predictors should include at least the number of equations. Remember to consider interaction! 

summary(EquationCitations)


```

```{r}

data("EquationCitations", package = "AER")
summary(EquationCitations)

# Install and load corrplot
#install.packages("corrplot")
library(corrplot)

# Create a correlation matrix
p.cor<-cor(EquationCitations[,3:13])

# Plot it
corrplot.mixed(p.cor)

```

```{r}

```


## Further resources

- [Ed Rubin](https://twitter.com/edrubin) has outstanding [teaching notes](http://edrub.in/teaching.html) for econometrics with R on his website. This includes both [undergrad-](https://github.com/edrubin/EC421W19) and [graduate-](http://edrub.in/ARE212/notes.html)level courses. I believe that he is turning these notes into a book with some coauthors, so stay tuned.
- Speaking of books, several introductory texts are freely available, including [*Introduction to Econometrics with R*](https://www.econometrics-with-r.org/) (Christoph Hanck *et al.*) and [*Using R for Introductory Econometrics*](http://www.urfie.net/) (Florian Heiss).
- [Tyler Ransom](https://twitter.com/tyleransom) has a nice [cheat sheet](https://github.com/tyleransom/EconometricsLabs/blob/master/tidyRcheatsheet.pdf) for common regression tasks and specifications.
- [Itamar Caspi](https://twitter.com/itamarcaspi) has written a neat unofficial appendix to this lecture, [*recipes for Dummies*](https://itamarcaspi.rbind.io/post/recipes-for-dummies/). The title might be a little inscrutable if you haven't heard of the `recipes` package before, but basically it handles "tidy" data preprocessing, which is an especially important topic for machine learning methods. We'll get to that later in course, but check out Itamar's post for a good introduction.
- I promised to provide some links to time series analysis. The good news is that R's support for time series is very, very good. The [Time Series Analysis](https://cran.r-project.org/web/views/TimeSeries.html) task view on CRAN offers an excellent overview of available packages and their functionality.


# FUTURE WORK 

## What we will talk about in the future - web scraping, pulling in data from APIs

# PART 1: Getting Started


## Working With (Real) Local Data

Now you have the basics in R. But we really want to learn how to import data! So let's do that. We have a csv file of the Atlantic hurricane database (HURDAT2) released by the **National Hurricane Center**. There is a pdf file providing metadata on the dataset. 

### Importing the Data...

The function `read_csv()` is a function to...read CSV (Comma-seperated values) files. This exists as another package. 

>>> What is a package? Packages are like **apps on a phone**, where the package is an app that adds functionality to our phone (RStudio). For example, if I want to post picutures to instagram, I have to install the Instagram package (app) on my phone.

## Installing Packages 

```{r}
#Install packages (like installing apps)
#install.packages("plyr")
#install.packages("ggplot2")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("request")
#install.packages("leaflet")
#install.packages("janitor")
```
Note that we installed multiple packages. We will use all of them in this workshop, but only focus on the `plyr` package for now.

Next, we need to add the package to our library, so we could acutally use it.
How do we do this? Let's try library:

## Call a Library

```{r}
?library()
library(plyr)
```
But where are the files we want to import? We can use `dir()` and `getwd()` to find out.

```{r}
getwd() #The current location where R is looking for files 
dir() #The files in the working directory.
```

## Set the Working Dir

We need to set the working directory in order to be sure RStudio can see the data. You should have downloaded this notebook and the data (atlantic.csv) to the same location. Be sure that they are right next to each other.

You can go to Session > Set Working Directory > Choose Directory to choose the FOLDER that contains your data (atlantic.csv). 

Now we can check. Click on the Files tab on the right hand side of the page, and click More > Go To Working Directory.

If we have the right working directory, we can import data. 


## Import Data 

The function `library()` has called the readr package into our library. Now we can use `read_csv()`.  We can make sure we are using the correct package by typing package::function, e.g. `readr::read_csv()`.

```{r}
hurrdata <- readr::read_csv("atlantic.csv")
```

>>> Note: If RStudio tells you "atlantic.csv not found" or "file not found" this means that it cannot find the file. RStudio looks for files in your working directory. Be sure that atlantic.csv is available next to the Intro_R_Script.R file, and go to: **Session > Set Working Directory > To Source File Location**. This tells RStudio to look for files next to this file. 

We see that a new variable, hurrdata, has been added to the environment.
```{r}
head(hurrdata)
```


This shows us a preview of the fist couple rows of the file, since it is quite large. 


## Investigate Data

We can also look at a summary.

```{r}
summary(hurrdata)
```


## Data Cleaning - Dates and Strings

Note how the date is stored in YYYYMMDD format. While this notation is great for sorting data based on date, it is not that convenient when we are only interested in the year or month.

Our goal is to extract year and month and store them in separate columns, based on the Date value.

```{r}
date.s <- as.character(hurrdata$Date)  #Creates a character string of the Data
hurrdata$Year <- substr(date.s,1,4)    #Extracts the year from the date string (position 1-4)
hurrdata$Month <- substr(date.s,5,6)   #Extracts the month from the date string (position 5-6)
hurrdata$Year <- as.numeric(as.character(hurrdata$Year)) #Convert the variable Year to numeric so we can easily plot it later
head(hurrdata)                       #prints the new data

```

New columns are added to the very end of the table, so you will have to scroll to the right to see them.

>>> You can also click on the data under Environment > Data to view it.

We might wonder - what type of data is hurricane data? We imported it from csv, but how is it stored? We can use the function `class()` to answer this. 

```{r}
class(hurrdata)
```


>>> YOU TRY: Try typing: class(hurrdata) in the console below. 

We can see that it's a data.frame and a "tbl" (table). A data frame is a table of data, where you have observations as rows and variable names as columns. Data frames have some great features for working with data, and are the classic R data storage.


## Variable Names

Did you notice the dollar sign `$` when formatting the date? This is a special operator that allows us to access variables (columns) based on their name.

To get a list of all the column names, we can use ... `names()`.

```{r}
names(hurrdata)
```

Let's say we want to take a closer look at only one variable. How about maximum wind speed? We can use the `$` operator to access only that 

```{r, error = TRUE}
#hurrdata$Maximum Wind
```

Oh no! Our frst **ERROR**! A quick Google search reveals that this error could be due to a missing quote mark. A look back at the variable names reveals this to be the case indeed - Maximum Wind contains a space, which means we should have included it in quotation marks for R to understand it.

However, quotation marks are easy to miss and could make our further analysis more inconvenient. Plus, a lot of other programs do not support spaces in variable names at all. Thus, it is good data management practice to ensure your variable names contain no spaces. It is common practise to replace spaces with periods `.` or underscores `_` instead. Luckily R contains a function that can do this for us.

```{r}
names(hurrdata) <- make.names(names(hurrdata))
head(hurrdata)  
```

See how all the spaces in variable names have been replaced by periods. Now we can access variables without worrying about quatation marks or spaces.

For example, we could take a look at the ten highest wind speeds;

```{r}
sort(hurrdata$Maximum.Wind, decreasing = TRUE)[1:10]
```

>>> Why did I include 'decreasing = TRUE' in the function call? Find out yourself by typing ?sort in the console.


## Make a Scatter Plot

To make graphs, we will load yet another library called ggplot2. It's great for data visualization!

```{r}
library(ggplot2)
```

Let's say we want to plot our hurricane data and look at how maximum windspeed has changed over time.

```{r}
hurrgraph <- ggplot(data = hurrdata, aes(x = Year, y = Maximum.Wind)) + # We list out the data, and then we can list different plots we want. 
  # geom_line(linetype = "dashed") + # This would make it a line graph, but we have too many points for that currently.  
  geom_point()

hurrgraph
```

Wow, this doesn't look very good.


## Cleaning the Data

We see we have negative data. If we looked into the metadata pdf, we would find that we should have removed these! We can check on the negative data:

```{r}
min(hurrdata$Maximum.Wind)
```

We know that `[]` can be used to access a row by its number. However, it can also be used with a condition to access a specific set of rows.
Knowing this, we can find all the negative (< 0) wind values and replace them with "NA" which means "No Data" in R. 

```{r}
hurrdata$Maximum.Wind[hurrdata$Maximum.Wind < 0] <- NA
#hurrdata <- hurrdata[hurrdata$Maximum.Wind<=0, ] # We could also delete the whole observation, but this is bad practice!
```

Let's check on the results. It should print "NA".

```{r}
min(hurrdata$Maximum.Wind)

```

We can also sample the data.frame so it doesn't take so long to load. We need to load another library (dplyr) to do this data manipulation.
```{r}
library(dplyr) #Always load dplyr after plyr, as I have.
hurrdata2 <- sample_n(hurrdata, 200, weight = NULL, replace =TRUE)
```

Now we re-run the graph, and modify the axis to make the Year easier to see. 

```{r}
hurrgraph2 <- ggplot(data = hurrdata2, aes(x = Year, y = Maximum.Wind, color = Maximum.Wind)) + 
  # geom_line(linetype = "dashed")+ # Again, we have too many points for a line graph
  geom_point() + # This adds the initial points
  theme(axis.text.x = element_text(angle = 45,hjust = 1,vjust = 0.5)) +
  scale_color_gradient(low = "blue", high = "red") + # Create a color scale to show high wind speed
  theme(legend.position = 'bottom') + # Put the legend on the bottom
  ylab("Maximum Wind (knots)") + # Change the y-label
  ggtitle("Selected Annual Hurricane Data, 1851 - 2015") + # Add a title
  theme(plot.title = element_text(lineheight = 0.8, face = "bold")) # Modify the title
hurrgraph2
```


## Advanced Graphics - Optional

The benefit of R is you can control almost everything...and that's the drawback of R as well. Remember to reference Stack Overflow and the links I provided. 

For those who are looking for an interactive graph - try plot_ly. 

```{r}
library(plotly)

hurrgraph3 <- plot_ly(hurrdata2, x = hurrdata2$Year, y = hurrdata2$Maximum.Wind, type = 'bar',
                     marker = list(color = 'rgb(158,202,225)', # This controls the color of the marker, which is the bar
                                   line = list(color = 'blue',
                                               width = 0.5))) %>%
  layout(title = "Selected Annual Hurricane Data, 1851 - 2015", # The title
         xaxis = list(title = "Year", tickangle = 40), # Axis labels. Notice the comma-seperation
         yaxis = list(title = "Maximum Reported Wind Speed (mph)"))

hurrgraph3
```




-- BREAK: What is tidy data? --




# PART 2: Introduction to Web Scraping and Tidy Data

The goals of this section are to discuss: 
* Batch downloads of data from the web (URL) using a *web scraping* approach.
* Organizing the data into a **tidy** data frame
* How to count how many values (observations) we have


```{r}
# Load the packages
library(readr)        # Used for reading in data
library(dplyr)        # A data management library
library(ggplot2)      # Used for graphing
library(janitor)      # Used for data cleaning
```


## *For* Loop to Download Files

The Department for Environment, Food & Rural Affairs (DEFRA) publishes annual concentrations of pollutants for local authorities in the United Kingdom.

For example, the 2010 dataset for PM 2.5 concentrations is located at https://uk-air.defra.gov.uk/datastore/pcm/popwmpm252010byUKlocalauthority.csv
The same dataset for the year 2015 is stored at https://uk-air.defra.gov.uk/datastore/pcm/popwmpm252015byUKlocalauthority.csv

A close investigation of the URLs reveals they they both follow the same format:
https://uk-air.defra.gov.uk/datastore/pcm/popwmpm25[YEAR]byUKlocalauthority.csv

Because the URLs of the each of the annual files have the same pattern we can write a *for loop* which downloads successive files, converts them into a tidy format, stacks the data and stores them in a data frame.

A data frame, as we mentioned before, is just a **virutal Excel sheet**.
For more information on data frames, see here: https://www.rdocumentation.org/packages/base/versions/3.6.1/topics/data.frame

We can use the `DT` package to create an interactive table embedded in document which stores the data and is sortable and searchable.


## Loops in R

To do this we need to make a loop. The general syntax, or method of writing, a loop is as follows:

for (value in list_of_values) {
  do something
}

Or, in R code: 

```{r}

for (year in 2010:2015) { 
  print(paste("The year is", year)) # This will print all the years from 2010 to 2015
}

```

Applying this knowledge, let's say we want to extract the data from 2010 to 2015. Looking at the URL, we know that the URL only changes based on the date. So, we need to paste in the URL and the year to change the date. 

We then use the pipe `%>%` to pass the data frame into four functions:

* mutate()
* select()
* gather()
* bind_rows()


```{r}
df <- data.frame()

for (year in 2010:2015) {
  
ap <- read_csv(paste0("https://uk-air.defra.gov.uk/datastore/pcm/popwmpm25", year,"byUKlocalauthority.csv"), skip = 2)
ap <- ap %>% mutate(year = year) %>% select(-`LA code`)
ap <- ap %>% tidyr::gather(indicator, value, 1:3)
df <- bind_rows(df, ap)

}

df <- janitor::clean_names(df, case = "snake") %>%
      arrange(local_authority, indicator, year)

DT::datatable(df)
```


We now have the data in as a data frame, `df` with 7254 observations of four variables. 


## Grouping Variables

We can also count the number of values per year using the `group_by()` function.

```{r}
df %>%
  group_by(year) %>%
  count()
```


## Create Faceted Plot

Finally, we have group the data into different tables by geographic area, the year of measurement, and the air pollution indicator which is being measured. From there, in each grouped table, we can calculate the mean PM 2.5 (particulate matter of 2.5 microns), and then create a trend chart.

There is a lot going on here:

1. We randomly select ten (out of over 400) local authoriries to include in our plot
2. We group the data by area, year and indicator
3. We calculate the mean pm25 values
4. We split the pm25 column into 3 parts
5. And drop the year (we already have a year column)
6. Then exclude the total values - just plot the non-anthropogenic and anthropogenic pm25 values
7. Then create a trend chart of pm25 values over time
8. Create a chart for each local authority

Note that if we would have included all of the local authorities, our plot would have been illegible. Plus it would have taken a long time to create.
If you wish, you can increase the amount of local autohories sampled, or even invlude all of them (by removing the filer line).
However, make sure to adjust the figure height `fig.height` accordingly when you do so 

```{r, fig.width = 10, fig.height = 20}
df %>%
  filter(local_authority %in% sample(unique(df$local_authority), 10)) %>%
  group_by(local_authority, year, indicator) %>% 
  summarise(meanvals = mean(value, na.rm = TRUE)) %>%
  tidyr::separate(indicator, c("pm", "year1", "type"), sep = " ") %>%
  select(-year1) %>% 
  filter(type != "(total)" & year > 2010) %>%
  ggplot(aes(x = year, y = meanvals, color = type)) +
    geom_line() +
    facet_grid(rows = vars(local_authority))
```
  

  
  
  
# PART 3: Using an Application Program Interface (API)

The goals of this section are to discuss: 
* Accessing data via an **API**
* How to deal with georeferenced data
* Creating interactive maps


```{r}
# Load the packages
library(dplyr)      # A data management library
library(tidyr)      # Used for tidying data
library(request)    # Used for HTTP GET requests
library(leaflet)    # A library for interacitve maps
```


## Using an API

The Environmental Protection Agency (EPA) makes real-time, historical, and forecasted air quality data available to developers and scientists via the AirNow API. We will use this API to investigate air quality in Massachusetts during rush hour (6 - 11 am) this morning.

**Before porceeding** go to https://docs.airnowapi.org/ to request an AirNow API account.

We will be using the `request` library to access data trhough this API. This library provides a painless interface for communicating with APIs where one does not have to worry about prasing URI strings and reading HTTP status codes, and extracting data from a JSON string.

The API that best fits our needs is the Observations by Monitoring Site API as it allows us to specify a precise geograpical area via a bounding box.
We will refer to the documentation of this API at https://docs.airnowapi.org/Data/docs to construct our query.

There are a couple crucial things to note when constructing our query:
1. The coordnates of the bounding box must come in a specific order (see documentation)
2. The start and end times and dates are in UTC, not EST (or EDT)
3. *You must use your own API key*

**Before proceeding** make sure to replace 'INSERT YOUR API KEY HERE' with your own API key!

```{r,error=TRUE}
res <- api("https://airnowapi.org/aq/data") %>%
  api_query(bbox = '-73.5,41.3,-69.9,42.8',
            startdate = '2019-10-07T11:00',
            enddate = '2019-10-07T16:00',
            parameters = 'pm25',
            datatype = 'C',
            format = 'application/json',
            api_key = 'INSERT YOUR API KEY HERE',
            verbose = 0,
            nowcastonly = 0,
            includerawconcentrations = 0) %>%
  http()
```

>>> Take a look at the response by typing res into the console or by double-clicking on it in the environment. What type is it?


## Formatting and Anlyzing the Response

The following is good practise using varius tidyverse libraries and the pipe `%>%` operator.
Here is what's happening:
1. We convert the response from a list into a dataframe
2. We generate an unique ID for each location by combinign the latitue and longitude
3. We filter our the descriptive fields we are not inrerested in
4. Then we group the data by location and calculate the mean pm25 value for each location
5. Finally we extract the geographical coordinates from the location ID we generated

```{r,error=TRUE}
pmdata <- res %>%
  ldply(data.frame) %>%
  mutate(latlon = paste(Latitude, Longitude)) %>%
  select(latlon, Value) %>%
  group_by(latlon) %>%
  summarize(value = mean(Value)) %>%
  separate("latlon", c("lat", "lon"), sep = " ", remove = TRUE, convert = TRUE)
```


## Mapping the Data

Finally we create an interactive map of our data using Leaflet - a JavaScript library for interactive maps. However, we will not be using JavaScript.
Leaflet, like many other popular JavaScript and Python libraries have a community-developed wrappers, allowing them to be used in R.

For mor information go to https://leafletjs.com/ and https://rstudio.github.io/leaflet/


Before we create the map, we will generate a yellow-red color scale to fit our pm25 values

```{r,error=TRUE}
pal <- colorNumeric(
  palette = "YlOrRd",
  domain = pmdata$value)
```


Creating a leaflet map is very similar to plotting with ggplot - it's all about layers.
First we must add a basemap, and then we can add our datapoints on top.

```{r,error=TRUE}
m = leaflet(pmdata) %>%
  addProviderTiles(providers$CartoDB.DarkMatter) %>%
  addCircles(lng = ~lon,
             lat = ~lat,
             radius = 3000,
             color = ~pal(value),
             popup = ~as.character(value),
             label = ~as.character(value),
             stroke = FALSE,
             fillOpacity = 0.5)
m
```

Feel free to play around with different basemaps:
https://rstudio.github.io/leaflet/basemaps.html
https://leaflet-extras.github.io/leaflet-providers/preview/

You can also use an API to load an external basemap or your preference. Google Maps and Mapbox are two popular options:
https://cloud.google.com/maps-platform/maps/
https://docs.mapbox.com/api/maps/


OPTIONAL: For more information on the `~` operator, check out the following:
https://www.r-bloggers.com/the-r-formula-method-the-good-parts/
https://www.r-bloggers.com/the-r-formula-method-the-bad-parts-2/


